{"name":"pyhsmm","body":"# Sampling Inference for Bayesian HSMMs and HMMs #\r\n`pyhsmm` is a Python library for approximate unsupervised sampling inference in\r\nBayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov\r\nModels (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM\r\nand HDP-HSMM, via the weak-limit approximation.\r\n\r\n<!--\r\nIn the Bayesian paradigm, inference refers to both what would in other contexts\r\nbe called \"learning\" (or \"parameter fitting\") as well as \"inference\": all the\r\nlatent variables in the model, including hidden states and transition/emission\r\nparameters, are included in the posterior distribution. The goal of sampling\r\ninference is to produce (approximate) samples from the posterior, and each\r\nsample roughly represents an alternative HMM or HSMM to explain the data. Using\r\nthe Bayesian Nonparametric HDP-HMM and HDP-HSMM, the sampled models that come\r\nout can be of different complexity: there may be good explanations using only 5\r\nstates as well as good explanations that use 15 states. The purpose of this\r\nsampling code is to produce samples of those alternatives.\r\n-->\r\n\r\n## Installing ##\r\nYou can clone this library with the usual command:\r\n\r\n```\r\ngit clone git://github.com/mattjj/pyhsmm.git\r\n```\r\n\r\nThe library depends on `numpy`, `scipy`, and, for visualization, `matplotlib`.\r\n\r\nThere is an optional dependency on the [Eigen C++ Template\r\nLibrary](http://eigen.tuxfamily.org/index.php?title=Main_Page) installed in the\r\nusual location of `/usr/local/include` (its default install location), which\r\ncan make the inference run much faster in many cases (and only a bit faster in\r\nothers). If you install Eigen, you can enable its use by calling\r\n`pyhsmm.use_eigen()` after importing `pyhsmm`.\r\n\r\n## A Simple Demonstration ##\r\nHere's how to draw from the HDP-HSMM posterior over HSMMs given a sequence of\r\nobservations. (The same example, along with the code to generate the synthetic\r\ndata loaded in this example, can be found in `examples/basic.py`.)\r\n\r\nLet's say we have some 2D data in a data.txt file:\r\n\r\n```bash\r\n$ head -n 5 data.txt\r\n1.954325679401778038e+00 -2.556951835061773703e+00\r\n5.112035450336103182e+00 -1.140375513299447086e+01\r\n3.746172989570439871e-01 1.437014805219438696e+00\r\n2.665840201248884433e+00 -6.032284411998907636e+00\r\n1.434245756651063797e+00 -3.241457044646912422e+00\r\n```\r\n\r\nIn Python, we can plot the data in a 2D plot, collapsing out the time dimension:\r\n\r\n```python\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\n\r\ndata = np.loadtxt('data.txt')\r\nplt.plot(data[:,0],data[:,1],'kx')\r\n```\r\n\r\n![2D data](http://www.mit.edu/~mattjj/github/pyhsmm2/data.png)\r\n\r\nWe can also make a plot of time versus the first principal component:\r\n\r\n```python\r\nfrom pyhsmm.util.plot import pca_project_data\r\nplt.plot(pca_project_data(data,1))\r\n```\r\n\r\n![Data first principal component vs time](http://www.mit.edu/~mattjj/github/pyhsmm2/data_vs_time.png)\r\n\r\nTo learn an HSMM, we'll use `pyhsmm` to create an hsmm object using some\r\nreasonable hyperparameters. We'll ask this model to infer the number of states\r\nas well (since an HDP-HSMM is instantiated by default), so we'll give it an\r\n`Nmax` parameter:\r\n\r\n```python\r\nimport pyhsmm\r\n\r\nobs_dim = 2\r\nNmax = 10\r\n\r\nobs_hypparams = {'mu_0':np.zeros(obs_dim),\r\n                'lmbda_0':np.eye(obs_dim),\r\n                'kappa_0':0.2,\r\n                'nu_0':obs_dim+2}\r\ndur_hypparams = {'k':8,\r\n                'theta':5}\r\n\r\nobs_distns = [pyhsmm.observations.gaussian(**obs_hypparams) for state in xrange(Nmax)]\r\ndur_distns = [pyhsmm.durations.poisson(**dur_hypparams) for state in xrange(Nmax)]\r\n\r\nposteriormodel = pyhsmm.hsmm(6.,6.,obs_distns,dur_distns,trunc=75)\r\n```\r\n\r\n(The first two arguments set the \"new-table\" proportionality constant for the\r\nmeta-Chinese Restaurant Process and the other CRPs, respectively, in the HDP\r\nprior on transition matrices. For this example, they really don't matter at\r\nall: pretty much any values will work; those parameters only matter in\r\nvery-low-evidence settings.)\r\n\r\nThe `trunc` parameter is an optional argument that can speed up inference: it\r\nsets a truncation limit on the maximum duration for any state. If you don't\r\npass in the `trunc` argument, no truncation is used and all possible state\r\nduration lengths are considered.\r\n\r\nThen, we add the data we want to condition on:\r\n\r\n```python\r\nposteriormodel.add_data(data)\r\n```\r\n\r\n(If we had multiple observation sequences to learn from, we could add them to the\r\nmodel just by calling `add_data()` for each observation sequence.)\r\n\r\nNow we run a resampling loop. For each iteration of the loop, all the latent\r\nvariables of the model will be resampled by Gibbs sampling steps, including the\r\ntransition matrix, the observation means and covariances, the duration\r\nparameters, and the hidden state sequence. We'll plot the samples every few\r\niterations.\r\n\r\n```python\r\nplot_every = 10\r\nfor idx in progprint_xrange(101):\r\n    if (idx % plot_every) == 0:\r\n        posteriormodel.plot()\r\n        plt.gcf().suptitle('inferred HSMM after %d iterations (arbitrary colors)' % idx)\r\n\r\n    posteriormodel.resample()\r\n```\r\n\r\n![Sampled models](http://www.mit.edu/~mattjj/github/pyhsmm2/posterior_animation.gif)\r\n\r\nI generated these data from an HSMM that looked like this:\r\n\r\n![Randomly-generated model and data](http://www.mit.edu/~mattjj/github/pyhsmm2/truth.png)\r\n\r\nSo the posterior samples look pretty good!\r\n\r\nIn fact, if you'd like to visualize why this example data isn't so tough for an HSMM to\r\ndecode, you can explore the 3D plot and see the strong time regularity with\r\n\r\n```python\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nfig = plt.figure()\r\nax = fig.add_subplot(111,projection='3d')\r\nax.scatter(np.arange(len(data)),data[:,0],data[:,1])\r\nplt.show()\r\n```\r\n\r\n## Speed ##\r\n\r\nHSMMs constitute a much more powerful model class than plain-old HMMs, and that\r\nenhanced power comes with a computational price: each sampling iteration for an\r\nHSMM is much slower than that of an HMM. But that price is often worthwhile if\r\nyou want to place priors on state durations or have the model learn duration\r\nstructure present in the data. (In the example, strong duration structure is\r\nwhat made the inference algorithm latch onto the correct explanation so\r\neasily.) In addition, the increased cost of each iteration often pays for\r\nitself, since HSMM samplers empirically seem to take fewer iterations to\r\nconverge than comparable HMM samplers.\r\n\r\nUsing my nothing-special i7-920 desktop machine and a NumPy/SciPy built against\r\nIntel's MKL BLAS (which generally outperforms ATLAS for vectorized operations)\r\nalong with `pyhsmm.use_eigen()`, here's how long the demo iterations took:\r\n\r\n```\r\n$ python -m pyhsmm.examples.basic\r\n.........................  [  25/101,    0.05sec avg,    3.95sec ETA ]\r\n.........................  [  50/101,    0.05sec avg,    2.64sec ETA ]\r\n.........................  [  75/101,    0.05sec avg,    1.34sec ETA ]\r\n.........................  [ 100/101,    0.05sec avg,    0.05sec ETA ]\r\n.\r\n   0.05sec avg,    5.21sec total\r\n```\r\n\r\n## Extending the Code ##\r\nTo add your own observation or duration distributions, implement the interfaces\r\ndefined in `abstractions.py`.\r\n\r\n## Contributors ##\r\nContributions by Chia-ying Lee.\r\n\r\n## References ##\r\n* Matthew J. Johnson and Alan S. Willsky, [Bayesian Nonparametric Hidden\r\nSemi-Markov Models](http://arxiv.org/abs/1203.1365). arXiv:1203.1365v1\r\n\r\n* Matthew J. Johnson and Alan S. Willsky, [The Hierarchical Dirichlet Process\r\nHidden Semi-Markov Model](http://www.mit.edu/~mattjj/papers/uai2010.pdf). 26th\r\nConference on Uncertainty in Artificial Intelligence (UAI 2010), Avalon,\r\nCalifornia, July 2010.\r\n","tagline":"a python library for Bayesian inference in (HDP-)H(S)MMs","google":"UA-30545037-1","note":"Don't delete this file! It's used internally to help with page regeneration."}